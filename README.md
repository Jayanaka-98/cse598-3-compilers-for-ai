# CSE 598 (3) Compilers for AI

## Project Report (with @savini @zichen)

[PromptCompEval: Evaluation Methodology for Comparing Prompt Compilation Frameworks](https://github.com/Jayanaka-98/cse598-3-compilers-for-ai/blob/main/598_PromptCompEval.pdf)

## Vibe_logs for papers.

| #  | Paper Title | Paper Link | Vibelog |
|:--:|-------------|------------|:-------:|
| 1  | **DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines** | [DSPy](https://arxiv.org/pdf/2310.03714) | [Link](https://chatgpt.com/share/693dc1c1-5b90-800f-bca5-92fdaebb3984) |
| 2  | **GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning** | [GEPA](https://arxiv.org/abs/2507.19457) | [Link](https://github.com/Jayanaka-98/cse598-3-compilers-for-ai/blob/main/gepa.md)|
| 3  | **MTP: A Meaning-Typed Language Abstraction for AI-Integrated Applications** | [MTP](https://arxiv.org/abs/2405.08965) | [Link](https://chatgpt.com/share/693dc847-1afc-800f-906d-4d95f344640a) |
| 4  | **TVM: An Automated End-to-End Optimizing Compiler for Deep Learning** | [TVM](https://arxiv.org/abs/1802.04799) | [Link](https://chatgpt.com/share/693dcbed-d9c4-800f-bef3-3cacc918c6c4) |
| 5  | **Relay: A High-Level Compiler for Deep Learning** | [Relay](https://arxiv.org/abs/1904.08368) | [Link](https://chatgpt.com/share/693ddc1b-f658-800f-b0f5-2ba665c0228c) |
| 6  | **Ansor: Generating High-Performance Tensor Programs for Deep Learning** | [Ansor](https://arxiv.org/abs/2006.06762) | [Link](https://chatgpt.com/share/693de2b5-49c4-800f-9800-8cbe41a96627) |
| 7 | **PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode and Graph Compilation for DNNs** | [PyTorch 2](https://dl.acm.org/doi/10.1145/3620665.3640366) | [Link](https://chatgpt.com/share/693de5f2-358c-800f-afde-d70aa911288f) |
| 8 | **TorchBench: Benchmarking PyTorch with High API Surface Coverage** | [TorchBench](https://arxiv.org/abs/2304.14226) | [Link](https://chatgpt.com/share/693de901-9b20-800f-a43a-042c5530e476) |
| 9 | **TorchTitan: One-stop PyTorch Native Solution for Production-Ready LLM Pretraining** | [TorchTitan](https://arxiv.org/abs/2410.06511) | [Link](https://chatgpt.com/share/693dec71-b820-800f-bd96-daaad69892c2) |
| 10 | **ECLIP: Energy-efficient and Practical Co-Location of ML Inference Pipelines on GPUs** | [ECLIP](https://arxiv.org/abs/2506.12598) | [Link](https://chatgpt.com/share/693dec57-140c-800f-8079-3d03c674cac6) |
| 11 | **Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations** | [Triton](https://dl.acm.org/doi/10.1145/3315508.3329973) | [Link](https://chatgpt.com/share/693dec2d-63a4-800f-b49e-3922814cef1c) |
| 12 | **Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks** | [Geak](https://arxiv.org/abs/2507.23194) | [Link](https://chatgpt.com/share/693de6c9-a814-800f-8e21-573fd5491323) |
| 13 | **Operator Fusion in XLA: Analysis and Evaluation** | [OpFusion](https://arxiv.org/abs/2301.13062) | [Link](https://chatgpt.com/share/693de62d-ccd4-800f-a6a9-dc2f6fa05448) |
| 14 | **Memory Safe Computations with XLA Compiler** | [MemSafeXLA](https://arxiv.org/abs/2206.14148) | [Link](https://chatgpt.com/share/693de650-628c-800f-8693-07751c28bd14) |
| 15 | **MLIR: A Compiler Infrastructure for the End of Moore's Law** | [MLIR](https://arxiv.org/abs/2002.11054) | [Link](https://chatgpt.com/share/693de2a1-1598-800f-9e55-1feba983dd06) |
| 16 | **Glow: Graph Lowering Compiler Techniques for Neural Networks** | [Glow](https://arxiv.org/abs/1805.00907) | [Link](https://chatgpt.com/share/693de288-cae0-800f-91c1-11a4f35f1b65) |
| 17 | **Efficient Memory Management for Large Language Model Serving with PagedAttention** | [PagedAttention](https://arxiv.org/abs/2309.06180) | [Link](https://chatgpt.com/share/693de263-7c88-800f-9566-c9928a2947a3) |
| 18 | **Effective Memory Management for Serving LLMs with Heterogeneity** | [EffLLMServ](https://arxiv.org/abs/2503.18292) | [Link](https://chatgpt.com/share/693ddc82-6084-800f-b5bb-110f00cf93b4) |
| 19 | **Demystifying the NVIDIA Ampere Architecture through Microbenchmarking and Instruction-Level Analysis** | [NVIDIA Ampere](https://arxiv.org/abs/2208.11174) | [Link](https://chatgpt.com/share/693dd57a-8db4-800f-bf96-164ce5cf5714) |
| 20 | **Optimizing sDTW for AMD GPUs** | [AMD sDTW](https://arxiv.org/abs/2403.06931) | [Link](https://chatgpt.com/share/693dd68b-84cc-800f-a78c-c042d47a1af6) |
| 21 | **TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings** | [TPU v4](https://arxiv.org/abs/2304.01433) | [Link](https://chatgpt.com/share/693dd783-0098-800f-9b68-39808c22953c) |
| 22 | **MTIA: First Generation Silicon Targeting Meta's Recommendation Systems** | [MTIA](https://dl.acm.org/doi/pdf/10.1145/3579371.3589348) | [Link](https://chatgpt.com/share/693dd72f-1838-800f-aa60-a935ac282735) |
| 23 | **Machine Learning Fleet Efficiency: Analyzing and Optimizing Large-Scale Google TPU Systems with ML Productivity Goodput** | [ML Fleet Efficiency](https://arxiv.org/pdf/2502.06982) | [Link](https://chatgpt.com/share/693dd7dc-5af4-800f-8d96-1f5ac6734934) |

